{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ§ª Exercise 3.1 â€” Comparing `SelfAttention_v1` and `SelfAttention_v2`\n",
    "\n",
    "In this exercise, we compare two implementations of self-attention: one using raw `nn.Parameter` definitions (`SelfAttention_v1`) and another using `nn.Linear` layers (`SelfAttention_v2`). Since `nn.Linear` stores its weight matrices in transposed form, we must transfer the weights carefully to ensure both modules produce identical outputs.\n",
    "\n",
    "**Steps:**\n",
    "- Create instances of both attention modules.\n",
    "- Copy the weights from `SelfAttention_v2` to `SelfAttention_v1`, accounting for the transpose.\n",
    "- Verify that both implementations produce the same output for the same input.\n",
    "\n"
   ],
   "id": "636f56f2e61c461e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-03T17:17:45.520817Z",
     "start_time": "2025-04-03T17:17:45.508872Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# SelfAttention_v1 with manually defined weights\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_k):\n",
    "        super().__init__()\n",
    "        self.query = nn.Parameter(torch.rand(d_in, d_k))\n",
    "        self.key = nn.Parameter(torch.rand(d_in, d_k))\n",
    "        self.value = nn.Parameter(torch.rand(d_in, d_k))\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = x @ self.query\n",
    "        K = x @ self.key\n",
    "        V = x @ self.value\n",
    "        attn_scores = Q @ K.T / K.shape[-1]**0.5\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        return attn_weights @ V\n",
    "\n",
    "# SelfAttention_v2 using nn.Linear layers\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_k):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_in, d_k, bias=False)\n",
    "        self.key = nn.Linear(d_in, d_k, bias=False)\n",
    "        self.value = nn.Linear(d_in, d_k, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        attn_scores = Q @ K.T / K.shape[-1]**0.5\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        return attn_weights @ V\n",
    "\n",
    "# Set input and head size\n",
    "d_in, d_k = 4, 2\n",
    "\n",
    "# Instantiate both versions\n",
    "sa2 = SelfAttention_v2(d_in, d_k)\n",
    "sa1 = SelfAttention_v1(d_in, d_k)\n",
    "\n",
    "# Copy weights (transpose because Linear stores weights as [out, in])\n",
    "with torch.no_grad():\n",
    "    sa1.query.copy_(sa2.query.weight.T)\n",
    "    sa1.key.copy_(sa2.key.weight.T)\n",
    "    sa1.value.copy_(sa2.value.weight.T)\n",
    "\n",
    "# Check if outputs match\n",
    "x = torch.rand(3, d_in)\n",
    "out1 = sa1(x)\n",
    "out2 = sa2(x)\n",
    "\n",
    "print(\"Are outputs equal?\", torch.allclose(out1, out2, atol=1e-6))\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are outputs equal? True\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ§ª Exercise 3.2 â€” Returning 2-dimensional Embedding Vectors\n",
    "\n",
    "This task requires configuring the `MultiHeadAttentionWrapper` such that its output embedding vectors are 2-dimensional. The number of attention heads is fixed at `num_heads = 2`, so we adjust `d_out = 2` to evenly split across the heads.\n",
    "\n",
    "**Steps:**\n",
    "- Use `SelfAttention_v2` for the heads.\n",
    "- Set `d_out = 2` and `num_heads = 2` so each head outputs a 1D vector.\n",
    "- Verify that the final output shape is `[batch_size, 2]`.\n"
   ],
   "id": "421708ca31246b55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T17:08:12.832976Z",
     "start_time": "2025-04-03T17:08:12.820080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0\n",
    "        self.heads = nn.ModuleList([\n",
    "            SelfAttention_v2(d_in, d_out // num_heads)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "        self.proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        head_outputs = [head(x) for head in self.heads]\n",
    "        concat = torch.cat(head_outputs, dim=-1)\n",
    "        return self.proj(concat)\n",
    "\n",
    "# Change d_out from 4 to 2 to make final output 2-dimensional\n",
    "mha = MultiHeadAttentionWrapper(d_in=4, d_out=2, num_heads=2)\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "output = mha(x)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Should be [3, 2]\n"
   ],
   "id": "da679f35bfdb046f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ§ª Exercise 3.3 â€” Initializing GPT-2 Size Attention Modules\n",
    "\n",
    "In this exercise, we initialize a `MultiHeadAttentionWrapper` with the same configuration as the smallest GPT-2 model:\n",
    "\n",
    "- `embedding size` = 768\n",
    "- `number of heads` = 12\n",
    "\n",
    "**Steps:**\n",
    "- Set both `d_in` and `d_out` to 768.\n",
    "- Set `num_heads = 12` (each head will handle 64 dimensions).\n",
    "- Test the module with dummy input and confirm the output shape is `[batch_size, 768]`.\n"
   ],
   "id": "ced76378cf732f56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T17:08:25.292824Z",
     "start_time": "2025-04-03T17:08:25.257965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a MultiHeadAttention for GPT-2 base configuration\n",
    "gpt2_attention = MultiHeadAttentionWrapper(\n",
    "    d_in=768,\n",
    "    d_out=768,\n",
    "    num_heads=12\n",
    ")\n",
    "\n",
    "# Test it with a dummy input\n",
    "dummy_input = torch.rand(1, 768)  # One token, 768-dimensional\n",
    "output = gpt2_attention(dummy_input)\n",
    "\n",
    "print(\"GPT-2 attention output shape:\", output.shape)  # Should be [1, 768]\n"
   ],
   "id": "8dc65335fa2a8040",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 attention output shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d555d0fb45f439ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
