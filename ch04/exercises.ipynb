{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-15T13:49:08.644913Z",
     "start_time": "2025-04-15T13:48:28.520350Z"
    }
   },
   "source": [
    "# Exercise 4 Answers - Chapter 4 GPT Implementation Homework\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() \n",
    "target_dir = os.path.join(current_dir, \"01_main-chapter-code\")\n",
    "sys.path.append(target_dir)\n",
    "\n",
    "from gpt import GPTModel, TransformerBlock\n",
    "\n",
    "# ----------------------\n",
    "# Exercise 4.1: Count Parameters\n",
    "# ----------------------\n",
    "\n",
    "def count_parameters(cfg):\n",
    "    block = TransformerBlock(cfg)\n",
    "    model = GPTModel(cfg)\n",
    "    \n",
    "    ff_params = sum(p.numel() for p in block.ff.parameters())\n",
    "    att_params = sum(p.numel() for p in block.att.parameters())\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(\"--- Exercise 4.1 ---\")\n",
    "    print(f\"FeedForward Parameters: {ff_params:,}\")\n",
    "    print(f\"Multi-Head Attention Parameters: {att_params:,}\")\n",
    "    print(f\"Total GPTModel Parameters: {total_params:,}\")\n",
    "    print(f\"Estimated Memory: {total_params * 4 / (1024 * 1024):.2f} MB\\n\")\n",
    "\n",
    "# ----------------------\n",
    "# Exercise 4.2: Init Larger Models\n",
    "# ----------------------\n",
    "\n",
    "def init_larger_models():\n",
    "    print(\"--- Exercise 4.2 ---\")\n",
    "    variants = {\n",
    "        \"GPT-2 Medium\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "        \"GPT-2 Large\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "        \"GPT-2 XL\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "    }\n",
    "    \n",
    "    base_cfg = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False,\n",
    "    }\n",
    "\n",
    "    for name, spec in variants.items():\n",
    "        cfg = base_cfg.copy()\n",
    "        cfg.update(spec)\n",
    "        model = GPTModel(cfg)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        size_mb = total_params * 4 / (1024 * 1024)\n",
    "        print(f\"{name}: {total_params:,} params (~{size_mb:.2f} MB)\")\n",
    "    print()\n",
    "\n",
    "# ----------------------\n",
    "# Exercise 4.3: Dropout Split\n",
    "# ----------------------\n",
    "\n",
    "def new_dropout_config():\n",
    "    print(\"--- Exercise 4.3 ---\")\n",
    "    cfg = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_layers\": 12,\n",
    "        \"n_heads\": 12,\n",
    "        \"dropout_emb\": 0.1,\n",
    "        \"dropout_attn\": 0.1,\n",
    "        \"dropout_resid\": 0.1,\n",
    "        \"dropout_ff\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "    }\n",
    "    cfg[\"drop_rate\"] = 0.1\n",
    "    print(\"Updated cfg with separate dropout values.\\n\")\n",
    "    return cfg\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    default_cfg = {\n",
    "        \"vocab_size\": 50257,\n",
    "        \"context_length\": 1024,\n",
    "        \"emb_dim\": 768,\n",
    "        \"n_layers\": 12,\n",
    "        \"n_heads\": 12,\n",
    "        \"drop_rate\": 0.1,\n",
    "        \"qkv_bias\": False\n",
    "    }\n",
    "    count_parameters(default_cfg)\n",
    "    init_larger_models()\n",
    "    updated_cfg = new_dropout_config()\n",
    "    count_parameters(updated_cfg)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Exercise 4.1 ---\n",
      "FeedForward Parameters: 4,722,432\n",
      "Multi-Head Attention Parameters: 2,360,064\n",
      "Total GPTModel Parameters: 163,009,536\n",
      "Estimated Memory: 621.83 MB\n",
      "\n",
      "--- Exercise 4.2 ---\n",
      "GPT-2 Medium: 406,212,608 params (~1549.58 MB)\n",
      "GPT-2 Large: 838,220,800 params (~3197.56 MB)\n",
      "GPT-2 XL: 1,637,792,000 params (~6247.68 MB)\n",
      "\n",
      "--- Exercise 4.3 ---\n",
      "Updated cfg with separate dropout values.\n",
      "\n",
      "--- Exercise 4.1 ---\n",
      "FeedForward Parameters: 4,722,432\n",
      "Multi-Head Attention Parameters: 2,360,064\n",
      "Total GPTModel Parameters: 163,009,536\n",
      "Estimated Memory: 621.83 MB\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T13:49:08.713594Z",
     "start_time": "2025-04-15T13:49:08.711089Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4be149a0724b4b6e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
