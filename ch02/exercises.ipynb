{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercise One:",
   "id": "a9978fa9326f74ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise one\n",
    "\n",
    "Byte Pair Encoding of Unknown Words"
   ],
   "id": "c1962237cd93f09f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T23:50:20.844436Z",
     "start_time": "2025-03-12T23:50:19.831082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the tokenizer (we'll use OpenAI's \"cl100k_base\" tokenizer)\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Input text\n",
    "text = \"Akwirw ier\"\n",
    "\n",
    "# Encode the text to get token IDs\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "# Decode the token IDs back to the original text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "\n",
    "# Display results\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Decoded Text:\", decoded_text)\n"
   ],
   "id": "3ac5a8e9309919ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [32, 29700, 404, 86, 602, 261]\n",
      "Decoded Text: Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercise two\n",
    "\n",
    "Data loaders with different strides and context sizes \n"
   ],
   "id": "5de13a9b73df201a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T00:15:00.727942Z",
     "start_time": "2025-03-13T00:14:58.275066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the text into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader(txt, batch_size=4, max_length=256, stride=128):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Sample input text (replace with actual text file content)\n",
    "sample_text = \"In the heart of the city stood the old library, a relic from a bygone era. \" \\\n",
    "              \"Its stone walls bore the marks of time, and ivy clung tightly to its facade.\"\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_text = tokenizer.encode(sample_text)\n",
    "\n",
    "# Define embedding layers\n",
    "vocab_size = 50257  # GPT-2 vocabulary size\n",
    "output_dim = 256\n",
    "max_len = 4  # Context length\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(max_len, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Generate data loaders with different max_length and stride values\n",
    "dataloader_2_2 = create_dataloader(sample_text, batch_size=1, max_length=2, stride=2)\n",
    "dataloader_8_2 = create_dataloader(sample_text, batch_size=1, max_length=8, stride=2)\n",
    "\n",
    "# Collect batch data for display\n",
    "batches_2_2 = [(inp.tolist(), tgt.tolist()) for inp, tgt in dataloader_2_2]\n",
    "batches_8_2 = [(inp.tolist(), tgt.tolist()) for inp, tgt in dataloader_8_2]\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df_2_2 = pd.DataFrame(batches_2_2, columns=[\"Input Tokens\", \"Target Tokens\"])\n",
    "df_8_2 = pd.DataFrame(batches_8_2, columns=[\"Input Tokens\", \"Target Tokens\"])\n",
    "\n",
    "# Print results\n",
    "print(\"Batches (max_length=2, stride=2):\")\n",
    "print(df_2_2)\n",
    "print(\"\\nBatches (max_length=8, stride=2):\")\n",
    "print(df_8_2)\n",
    "\n"
   ],
   "id": "59bb6ce7b0bddeb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches (max_length=2, stride=2):\n",
      "       Input Tokens    Target Tokens\n",
      "0      [[818, 262]]    [[262, 2612]]\n",
      "1     [[2612, 286]]     [[286, 262]]\n",
      "2     [[262, 1748]]   [[1748, 6204]]\n",
      "3     [[6204, 262]]    [[262, 1468]]\n",
      "4    [[1468, 5888]]     [[5888, 11]]\n",
      "5       [[11, 257]]   [[257, 26341]]\n",
      "6    [[26341, 422]]     [[422, 257]]\n",
      "7      [[257, 416]]   [[416, 21260]]\n",
      "8   [[21260, 6980]]     [[6980, 13]]\n",
      "9      [[13, 6363]]   [[6363, 7815]]\n",
      "10   [[7815, 7714]]  [[7714, 18631]]\n",
      "11   [[18631, 262]]    [[262, 8849]]\n",
      "12    [[8849, 286]]     [[286, 640]]\n",
      "13      [[640, 11]]      [[11, 290]]\n",
      "14   [[290, 21628]]    [[21628, 88]]\n",
      "15      [[88, 537]]    [[537, 2150]]\n",
      "16  [[2150, 17707]]   [[17707, 284]]\n",
      "17     [[284, 663]]   [[663, 43562]]\n",
      "\n",
      "Batches (max_length=8, stride=2):\n",
      "                                         Input Tokens  \\\n",
      "0       [[818, 262, 2612, 286, 262, 1748, 6204, 262]]   \n",
      "1     [[2612, 286, 262, 1748, 6204, 262, 1468, 5888]]   \n",
      "2       [[262, 1748, 6204, 262, 1468, 5888, 11, 257]]   \n",
      "3      [[6204, 262, 1468, 5888, 11, 257, 26341, 422]]   \n",
      "4       [[1468, 5888, 11, 257, 26341, 422, 257, 416]]   \n",
      "5      [[11, 257, 26341, 422, 257, 416, 21260, 6980]]   \n",
      "6     [[26341, 422, 257, 416, 21260, 6980, 13, 6363]]   \n",
      "7     [[257, 416, 21260, 6980, 13, 6363, 7815, 7714]]   \n",
      "8   [[21260, 6980, 13, 6363, 7815, 7714, 18631, 262]]   \n",
      "9     [[13, 6363, 7815, 7714, 18631, 262, 8849, 286]]   \n",
      "10     [[7815, 7714, 18631, 262, 8849, 286, 640, 11]]   \n",
      "11     [[18631, 262, 8849, 286, 640, 11, 290, 21628]]   \n",
      "12        [[8849, 286, 640, 11, 290, 21628, 88, 537]]   \n",
      "13      [[640, 11, 290, 21628, 88, 537, 2150, 17707]]   \n",
      "14     [[290, 21628, 88, 537, 2150, 17707, 284, 663]]   \n",
      "\n",
      "                                        Target Tokens  \n",
      "0      [[262, 2612, 286, 262, 1748, 6204, 262, 1468]]  \n",
      "1       [[286, 262, 1748, 6204, 262, 1468, 5888, 11]]  \n",
      "2     [[1748, 6204, 262, 1468, 5888, 11, 257, 26341]]  \n",
      "3       [[262, 1468, 5888, 11, 257, 26341, 422, 257]]  \n",
      "4      [[5888, 11, 257, 26341, 422, 257, 416, 21260]]  \n",
      "5      [[257, 26341, 422, 257, 416, 21260, 6980, 13]]  \n",
      "6      [[422, 257, 416, 21260, 6980, 13, 6363, 7815]]  \n",
      "7   [[416, 21260, 6980, 13, 6363, 7815, 7714, 18631]]  \n",
      "8    [[6980, 13, 6363, 7815, 7714, 18631, 262, 8849]]  \n",
      "9    [[6363, 7815, 7714, 18631, 262, 8849, 286, 640]]  \n",
      "10      [[7714, 18631, 262, 8849, 286, 640, 11, 290]]  \n",
      "11        [[262, 8849, 286, 640, 11, 290, 21628, 88]]  \n",
      "12        [[286, 640, 11, 290, 21628, 88, 537, 2150]]  \n",
      "13      [[11, 290, 21628, 88, 537, 2150, 17707, 284]]  \n",
      "14   [[21628, 88, 537, 2150, 17707, 284, 663, 43562]]  \n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
